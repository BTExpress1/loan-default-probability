{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70c7010-d3af-4316-97cb-30f93bac79e5",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\" align = center> Imports</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f9a79-5c53-47fa-a4ff-028aa32d44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, subprocess, json, hashlib, shutil, glob, pathlib, time, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import chi2,mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "plt.style.use(\"seaborn-v0_8\")   # clean default style\n",
    "sns.set_palette(\"muted\")        # consistent colors\n",
    "\n",
    "#Start a timer to check the execution time of the notebook.\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fccfe-8a7d-4aeb-8807-e90a297d1215",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\" align = center> Functions</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9855351-a4ea-4358-ac96-f5b5ba43305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_default(status):\n",
    "    bad = {\"Charged Off\", \"Default\", \"Late (31-120 days)\", \"Late (16-30 days)\"}   # risky\n",
    "    good = {\"Fully Paid\"}                                                          # safe\n",
    "    if status in bad: \n",
    "        return 1\n",
    "    elif status in good: \n",
    "        return 0\n",
    "    else: \n",
    "        return np.nan   # ambiguous → drop later if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf7e50-6f10-4087-9e5c-b8e7785be82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_reduce(\n",
    "    df,\n",
    "    na_thresh: float = 0.99,         # drop cols with >99% NA\n",
    "    cat_freq_thresh: float = 0.5,    # object→category if unique_ratio < 0.5\n",
    "    add_missing_ind: bool = True,    # add __is_missing flags for NA-heavy cols\n",
    "    ind_thresh: float = 0.20,        # add indicator if >=5% missing\n",
    "    impute: bool = True,             # run SimpleImputer (no row drops)\n",
    "    fitted_imputer=None,             # pass a previously fitted imputer to reuse\n",
    "    return_artifacts: bool = True    # return metadata incl. dropped cols & imputer\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-savvy reducer + optional imputation without dropping rows.\n",
    "    Steps:\n",
    "      1) Drop cols with > na_thresh missing or constant.\n",
    "      2) Downcast numerics; object→category when repetitive.\n",
    "      3) Optionally add missingness indicators: <col>__is_missing (int8).\n",
    "      4) Optionally impute: median for numerics, most_frequent for categoricals.\n",
    "    Returns:\n",
    "      df_out, (optional) artifacts dict.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    n0, c0 = len(df), df.shape[1]\n",
    "\n",
    "    # 1) Drop NA-heavy + constant columns\n",
    "    na_frac = df.isna().mean()\n",
    "    drop_na = na_frac[na_frac > na_thresh].index.tolist()\n",
    "    drop_const = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n",
    "    dropped_cols = sorted(set(drop_na + drop_const))\n",
    "    df_red = df.drop(columns=dropped_cols) if dropped_cols else df.copy()\n",
    "\n",
    "    # 2) Downcast numerics; object→category (repetitive only)\n",
    "    num_cols_all = df_red.select_dtypes(include=\"number\").columns\n",
    "    for col in num_cols_all:\n",
    "        s = df_red[col]\n",
    "        if pd.api.types.is_float_dtype(s):\n",
    "            df_red[col] = s.astype(\"float32\")\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            df_red[col] = pd.to_numeric(s, downcast=\"integer\")\n",
    "\n",
    "    obj_cols_all = df_red.select_dtypes(include=\"object\").columns\n",
    "    n_rows = max(len(df_red), 1)\n",
    "    for col in obj_cols_all:\n",
    "        if df_red[col].nunique(dropna=False) / n_rows < cat_freq_thresh:\n",
    "            df_red[col] = df_red[col].astype(\"category\")\n",
    "\n",
    "    # 3) Add missingness indicators (no row drops)\n",
    "    indicators = []\n",
    "    if add_missing_ind:\n",
    "        miss = df_red.isna().mean()\n",
    "        for c, frac in miss.items():\n",
    "            if frac >= ind_thresh:\n",
    "                ind_name = f\"{c}__is_missing\"\n",
    "                df_red[ind_name] = df_red[c].isna().astype(\"int8\")\n",
    "                indicators.append(ind_name)\n",
    "\n",
    "    # Identify columns for imputation (exclude indicators)\n",
    "    num_cols = df_red.select_dtypes(include=[\"number\", \"float32\", \"float64\", \"int32\", \"int64\"]).columns.tolist()\n",
    "    cat_cols = df_red.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if not c.endswith(\"__is_missing\")]\n",
    "    cat_cols = [c for c in cat_cols if not c.endswith(\"__is_missing\")]\n",
    "\n",
    "    # 4) Impute (median for numeric, most_frequent for categorical)\n",
    "    imputer_pipe = None\n",
    "    if impute:\n",
    "        num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        pre = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_imputer, num_cols),\n",
    "                (\"cat\", cat_imputer, cat_cols),\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        if fitted_imputer is None:\n",
    "            imputer_pipe = Pipeline([(\"imputer\", pre)])\n",
    "            X_imp = imputer_pipe.fit_transform(df_red)\n",
    "        else:\n",
    "            imputer_pipe = fitted_imputer\n",
    "            X_imp = imputer_pipe.transform(df_red)\n",
    "\n",
    "        # rebuild dataframe in same order, then reattach indicators\n",
    "        imp_cols = num_cols + cat_cols\n",
    "        df_out = pd.DataFrame(X_imp, columns=imp_cols, index=df_red.index)\n",
    "\n",
    "        # cast back categories\n",
    "        for c in cat_cols:\n",
    "            df_out[c] = df_out[c].astype(\"category\")\n",
    "\n",
    "        # reattach indicators\n",
    "        for ind in indicators:\n",
    "            df_out[ind] = df_red[ind].astype(\"int8\")\n",
    "    else:\n",
    "        df_out = df_red\n",
    "        imputer_pipe = None\n",
    "\n",
    "    artifacts = {\n",
    "        \"dropped_cols\": dropped_cols,\n",
    "        \"indicators_added\": indicators,\n",
    "        \"num_cols\": num_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"imputer\": imputer_pipe,\n",
    "        \"rows_before\": n0,\n",
    "        \"cols_before\": c0,\n",
    "        \"rows_after\": len(df_out),\n",
    "        \"cols_after\": df_out.shape[1],\n",
    "    }\n",
    "\n",
    "    return (df_out, artifacts) if return_artifacts else df_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef1064-1d23-48f9-aaeb-ddd2993b4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weak_categoricals(df: pd.DataFrame, target: str, min_freq: int = 2):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with categorical features ranked by signal:\n",
    "      - chi2_min_p: min p-value across one-hot levels (lower is stronger)\n",
    "      - mi_mean: mean mutual information across one-hot levels (higher is stronger)\n",
    "\n",
    "    min_freq: levels with count < min_freq are collapsed into '__rare__' to stabilize stats.\n",
    "    \"\"\"\n",
    "    # 0) checks\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"target '{target}' not in DataFrame columns\")\n",
    "    if df[target].isna().any():\n",
    "        # simple fix: drop NAs in target only (no row imputation here)\n",
    "        df = df.loc[~df[target].isna()].copy()\n",
    "\n",
    "    y = LabelEncoder().fit_transform(df[target])\n",
    "\n",
    "    # 1) collect categorical columns (exclude target + indicators)\n",
    "    cat_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
    "    cat_cols = [c for c in cat_cols if c != target and not c.endswith(\"__is_missing\")]\n",
    "\n",
    "    if not cat_cols:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"chi2_min_p\",\"mi_mean\",\"n_levels\",\"kept_levels\"])\n",
    "\n",
    "    # 2) sanitize each categorical: collapse rare levels\n",
    "    df_cat = df[cat_cols].copy()\n",
    "    for c in cat_cols:\n",
    "        s = df_cat[c].astype(\"category\")\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        rare = vc[vc < min_freq].index\n",
    "        if len(rare) > 0:\n",
    "            s = s.replace(dict.fromkeys(rare, \"__rare__\")).astype(\"category\")\n",
    "        df_cat[c] = s\n",
    "\n",
    "    # 3) one-hot encode all cats (keep all levels to evaluate signal)\n",
    "    X = pd.get_dummies(df_cat, dummy_na=False)  # no drop_first; evaluate all levels\n",
    "\n",
    "    # guard: chi2 requires non-negative\n",
    "    if X.shape[1] == 0:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"chi2_min_p\",\"mi_mean\",\"n_levels\",\"kept_levels\"])\n",
    "\n",
    "    # remove zero-variance columns (can happen after rare collapsing)\n",
    "    nz_cols = [c for c in X.columns if X[c].nunique() > 1]\n",
    "    X = X[nz_cols]\n",
    "    if X.shape[1] == 0:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"chi2_min_p\",\"mi_mean\",\"n_levels\",\"kept_levels\"])\n",
    "\n",
    "    # 4) chi2 + MI\n",
    "    chi_stats, chi_pvals = chi2(X, y)\n",
    "    mi_vals = mutual_info_classif(X, y, discrete_features=True, random_state=42)\n",
    "\n",
    "    # 5) aggregate back to original feature names\n",
    "    #   one-hot columns look like \"<col>_<level>\"\n",
    "    def base_name(col):\n",
    "        # handle underscores inside categories by splitting from the right once\n",
    "        # pandas uses col + '_' + level; we reverse split one time\n",
    "        parts = col.rsplit(\"_\", 1)\n",
    "        return parts[0] if len(parts) == 2 else col\n",
    "\n",
    "    df_scores = pd.DataFrame({\n",
    "        \"oh_col\": X.columns,\n",
    "        \"chi_p\": chi_pvals,\n",
    "        \"mi\": mi_vals\n",
    "    })\n",
    "    df_scores[\"feature\"] = df_scores[\"oh_col\"].map(base_name)\n",
    "\n",
    "    agg = (df_scores\n",
    "           .groupby(\"feature\")\n",
    "           .agg(chi2_min_p=(\"chi_p\",\"min\"),\n",
    "                mi_mean=(\"mi\",\"mean\"),\n",
    "                n_levels=(\"oh_col\",\"count\"))\n",
    "           .reset_index()\n",
    "           .sort_values([\"mi_mean\",\"chi2_min_p\"], ascending=[False, True]))\n",
    "\n",
    "    # keep a compact view of which levels survived (optional but handy)\n",
    "    kept_levels = (df_scores.groupby(\"feature\")[\"oh_col\"]\n",
    "                   .apply(lambda s: \", \".join(s.head(5)) + (\" ...\" if len(s) > 5 else \"\")))\n",
    "    agg[\"kept_levels\"] = agg[\"feature\"].map(kept_levels)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a90da-a31f-4a72-a02f-63d0991be327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_default_prob_grid(\n",
    "    df, \n",
    "    features, \n",
    "    target=\"target_default\", \n",
    "    n_cols=3, \n",
    "    bins=6, \n",
    "    strategy=\"quantile\",        # \"quantile\" or \"uniform\"\n",
    "    max_cats=12,                # limit for wide categoricals\n",
    "    figsize_per_plot=(6.0, 4.2),\n",
    "    decimals=4                  # bin label precision when not integer-like\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Grid of P(target=1) plots:\n",
    "      - Numeric (non-binary): binned barplots with pretty bin labels\n",
    "      - Binary numeric & categoricals: category barplots\n",
    "    Skips missing columns; hides unused subplots.\n",
    "    \"\"\"\n",
    "    ok_feats = [f for f in features if f in df.columns]\n",
    "    miss_feats = [f for f in features if f not in df.columns]\n",
    "    if miss_feats:\n",
    "        print(\"Skipping missing columns:\", miss_feats)\n",
    "    if not ok_feats:\n",
    "        print(\"No valid features to plot.\")\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    n = len(ok_feats)\n",
    "    n_rows = int(np.ceil(n / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(figsize_per_plot[0]*n_cols, figsize_per_plot[1]*n_rows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    def _is_binary_numeric(s: pd.Series) -> bool:\n",
    "        return pd.api.types.is_numeric_dtype(s) and s.dropna().nunique() <= 2\n",
    "\n",
    "    def _is_integer_like(s: pd.Series) -> bool:\n",
    "        # true if integer dtype OR all non-null values are whole numbers\n",
    "        if pd.api.types.is_integer_dtype(s):\n",
    "            return True\n",
    "        v = s.dropna().values\n",
    "        if v.size == 0:\n",
    "            return False\n",
    "        return np.all(np.isfinite(v)) and np.all(np.abs(v - np.round(v)) < 1e-9)\n",
    "\n",
    "    for i, feat in enumerate(ok_feats):\n",
    "        ax = axes[i]\n",
    "        s = df[feat]\n",
    "\n",
    "        # Decide plotting mode\n",
    "        treat_as_numeric = pd.api.types.is_numeric_dtype(s) and not _is_binary_numeric(s)\n",
    "\n",
    "        if treat_as_numeric:\n",
    "            # Numeric → bin\n",
    "            if strategy == \"quantile\":\n",
    "                b = pd.qcut(s, q=bins, duplicates=\"drop\")\n",
    "            else:\n",
    "                b = pd.cut(s, bins=bins)\n",
    "\n",
    "            # Pretty bin labels\n",
    "            if _is_integer_like(s):\n",
    "                b = b.cat.rename_categories(lambda x: f\"{int(np.floor(x.left))} – {int(np.ceil(x.right))}\")\n",
    "            else:\n",
    "                fmt = f\"{{:.{decimals}f}} – {{:.{decimals}f}}\"\n",
    "                b = b.cat.rename_categories(lambda x: fmt.format(x.left, x.right))\n",
    "\n",
    "            # Compute default rate per bin\n",
    "            g = (\n",
    "                df[[target]]\n",
    "                .join(b.rename(\"__bin__\"))\n",
    "                .dropna(subset=[\"__bin__\"])\n",
    "                .groupby(\"__bin__\")[target]\n",
    "                .mean()\n",
    "                .reset_index(name=\"p_default\")\n",
    "            )\n",
    "\n",
    "            sns.barplot(x=\"__bin__\", y=\"p_default\", data=g, ax=ax)\n",
    "            ax.set_title(f\"{feat} (binned)\")\n",
    "            ax.set_xlabel(feat)\n",
    "            ax.set_ylabel(\"P(Default=1)\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "\n",
    "        else:\n",
    "            # Categorical or binary → category default rate\n",
    "            vc = df[feat].value_counts(dropna=False)\n",
    "            cats = vc.index[:max_cats]\n",
    "            g = (\n",
    "                df[df[feat].isin(cats)]\n",
    "                .groupby(feat, observed=True)[target]\n",
    "                .mean()\n",
    "                .reset_index(name=\"p_default\")\n",
    "                .sort_values(\"p_default\", ascending=False)\n",
    "            )\n",
    "            sns.barplot(x=feat, y=\"p_default\", data=g, ax=ax)\n",
    "            ax.set_title(f\"{feat}\")\n",
    "            ax.set_xlabel(feat)\n",
    "            ax.set_ylabel(\"P(Default=1)\")\n",
    "            ax.tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "\n",
    "    # Remove any unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35125ac3-b24d-4d0a-8ea0-686452f6d224",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\" align = center> Data Wrangling</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2356f-5b70-4831-9c17-fb71b91ea4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82c4efe1-b3a4-4558-97e6-2039de8fa68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:56:42.830125Z",
     "iopub.status.busy": "2025-09-05T19:56:42.830125Z",
     "iopub.status.idle": "2025-09-05T19:56:42.833151Z",
     "shell.execute_reply": "2025-09-05T19:56:42.833151Z",
     "shell.execute_reply.started": "2025-09-05T19:56:42.830125Z"
    }
   },
   "source": [
    "### <div class=\"alert alert-info\" > Load The Data</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b1393-4e47-443b-aeb0-a5e287b15034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart, idempotent fetch with checksum + metadata\n",
    "\n",
    "import subprocess, sys, hashlib, pathlib, glob, shutil, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Ensure kagglehub installed in THIS kernel\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"kagglehub\"], check=True)\n",
    "import kagglehub\n",
    "\n",
    "def sha256_of(path, chunk=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def write_meta_if_changed(meta_file: Path, meta: dict, change_keys=(\"cache_sha256\",\"local_sha256\",\"cached_csv\")):\n",
    "    \"\"\"Write metadata only when data changed. Keeps audit trail in .history.jsonl.\"\"\"\n",
    "    prev = {}\n",
    "    if meta_file.exists():\n",
    "        try: prev = json.loads(meta_file.read_text())\n",
    "        except: prev = {}\n",
    "    changed = (not prev) or any(prev.get(k) != meta.get(k) for k in change_keys)\n",
    "    if changed:\n",
    "        meta_file.write_text(json.dumps(meta, indent=2))\n",
    "        hist = meta_file.with_suffix(\".history.jsonl\")\n",
    "        with open(hist, \"a\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(json.dumps(meta) + \"\\n\")\n",
    "        print(f\"📝 Wrote metadata: {meta_file}\")\n",
    "    else:\n",
    "        print(f\"✅ Metadata unchanged: {meta_file}\")\n",
    "\n",
    "# 1) Repo layout\n",
    "proj = pathlib.Path().cwd()\n",
    "raw_dir = proj / \"data\" / \"raw\"\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "local_file = raw_dir / \"loan_default_probability_raw.csv\"\n",
    "meta_file  = raw_dir / \".dataset_meta.json\"\n",
    "\n",
    "# 2) Download to cache (kagglehub caches under user dir)\n",
    "cache_path = kagglehub.dataset_download(\"adarshsng/lending-club-loan-data-csv\")\n",
    "cache_path = pathlib.Path(cache_path)\n",
    "\n",
    "# Pick the CSV we need from cache\n",
    "candidate = None\n",
    "for f in glob.glob(str(cache_path / \"*.csv\")):\n",
    "    # adjust filter as needed to target the main loan-level table\n",
    "    if \"loan\" in f.lower() and \"dictionary\" not in f.lower():\n",
    "        candidate = pathlib.Path(f)\n",
    "        break\n",
    "if candidate is None:\n",
    "    raise FileNotFoundError(\"Expected loan CSV not found in downloaded dataset.\")\n",
    "\n",
    "# 3) Compare checksum; copy only if different or local missing\n",
    "cache_hash = sha256_of(candidate)\n",
    "local_hash = sha256_of(local_file) if local_file.exists() else None\n",
    "\n",
    "if local_hash == cache_hash:\n",
    "    print(f\"✅ Up-to-date: {local_file}\")\n",
    "else:\n",
    "    shutil.copy(candidate, local_file)\n",
    "    print(f\"⬇️  Updated: {local_file}\")\n",
    "\n",
    "# 4) Write metadata (only if changed)\n",
    "meta = {\n",
    "    \"source\": \"kagglehub:adarshsng/lending-club-loan-data-csv\",\n",
    "    \"cached_csv\": str(candidate),\n",
    "    \"cache_sha256\": cache_hash,\n",
    "    \"local_path\": str(local_file),\n",
    "    \"local_sha256\": sha256_of(local_file),\n",
    "    \"fetched_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "# Try to capture version from cache path (if present)\n",
    "parts = [p for p in candidate.parts if p.lower().startswith(\"v\") and p[1:].isdigit()]\n",
    "if parts:\n",
    "    meta[\"dataset_version_hint\"] = parts[0]\n",
    "\n",
    "write_meta_if_changed(meta_file, meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ff8a6-55d8-458e-895c-436afc10841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default = pd.read_csv(local_file,nrows=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d58004-0800-4750-90b3-d61a271ddcdb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" ><strong>We are going to look into the details of our data.</strong><br><br>\n",
    "\n",
    "This includes info(), describe(), shape, checking for nulls, and more</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b603338-f07f-4a97-ac02-48ae8548eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c69448-70cd-4fe4-8ac8-6ea3228ccc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951318a-ec04-4c0f-9703-8850973b01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3508a7-1bff-47fa-9528-4b0158fdcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_default.isna().sum().sum()     # confirm true null count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351eea1-871f-49f3-8b19-930fcdc0b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAMPLE = True\n",
    "parq_in = \"data/processed/loan_default_slim.parquet\"\n",
    "df_in = pd.read_parquet(parq_in).sample(frac=0.1, random_state=42) if USE_SAMPLE else pd.read_parquet(parq_in)\n",
    "\n",
    "# First pass (fit imputer on sample)\n",
    "df_imp, art = clean_and_reduce(df_in, na_thresh=0.95, ind_thresh=0.05, impute=True)\n",
    "pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
    "joblib.dump(art[\"imputer\"], \"models/imputer_simple.joblib\")\n",
    "\n",
    "# Save output\n",
    "out_path = \"data/processed/loan_default_imputed_sample.parquet\" if USE_SAMPLE else \"data/processed/loan_default_imputed_full.parquet\"\n",
    "df_imp.to_parquet(out_path, index=False)\n",
    "\n",
    "# Later on full data: reuse the same imputer for consistent treatment\n",
    "if not USE_SAMPLE:\n",
    "    from joblib import load\n",
    "    imputer = load(\"models/imputer_simple.joblib\")\n",
    "    df_imp_full, art_full = clean_and_reduce(\n",
    "        df_in, na_thresh=0.95, ind_thresh=0.05, impute=True, fitted_imputer=imputer\n",
    "    )\n",
    "    df_imp_full.to_parquet(\"data/processed/loan_default_imputed_full.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594c940-5304-47ab-a06f-62357fc9d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35ecbe-15e5-406d-a5e5-0d8cca5f2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c82db5-be8c-4e18-bb23-5494be5de121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5eb0d8-63d2-41e2-8740-618cb538aa80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" ><strong> Now that we have a cleaner dataset</strong><br>\n",
    "We need to check for categorical features that add no signal, no siginificance to our analysis.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a680140-0e0f-4e2a-ae3c-2721bd650587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature signal check\n",
    "agg = find_weak_categoricals(df_imp, target=\"loan_status\", min_freq=5)\n",
    "\n",
    "# Define weak threshold (tune as you like)\n",
    "weak = agg[(agg[\"chi2_min_p\"] > 0.9) & (agg[\"mi_mean\"] < 1e-4)]\n",
    "\n",
    "print(f\"Identified {len(weak)} weak categorical features to drop\")\n",
    "print(weak[[\"feature\",\"chi2_min_p\",\"mi_mean\"]].head())\n",
    "\n",
    "# Drop them from df\n",
    "df_pruned = df_imp.drop(columns=weak[\"feature\"].tolist(), errors=\"ignore\")\n",
    "\n",
    "print(\"Before:\", df_imp.shape, \"After:\", df_pruned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad605b15-080a-4829-84bb-f07f148fc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"data/processed/loan_default_pruned_sample.parquet\" if USE_SAMPLE else \"data/processed/loan_default_pruned_full.parquet\"\n",
    "df_pruned.to_parquet(out_path, index=False)\n",
    "print(f\"✅ Saved pruned dataset: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee1341-8399-4aee-a725-4a213dc0784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Select numeric columns only\n",
    "num_df = df_pruned.select_dtypes(include=[\"number\",\"float32\",\"float64\",\"int32\",\"int64\"])\n",
    "\n",
    "# 2) Compute correlation matrix (Spearman handles skewed/ordinal better than Pearson)\n",
    "corr = num_df.corr(method=\"spearman\")\n",
    "\n",
    "# 3) Mask upper triangle (to make heatmap readable)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .6})\n",
    "plt.title(\"Numeric Feature Correlation Heatmap\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b60568-d92e-4a7a-8f42-9f096b9ebe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for high correlation\n",
    "THRESH = 0.85\n",
    "to_drop = set()\n",
    "\n",
    "corr_matrix = corr.abs()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i,j] > THRESH:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            to_drop.add(colname)\n",
    "\n",
    "print(f\"Suggest dropping {len(to_drop)} highly correlated features:\")\n",
    "print(list(to_drop)[:15])\n",
    "\n",
    "df_uncorr = df_pruned.drop(columns=list(to_drop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1f696-5148-4caf-989c-e87fc68d28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_corr = [\n",
    "    'sec_app_inq_last_6mths__is_missing',\n",
    "    'sec_app_collections_12_mths_ex_med__is_missing',\n",
    "    'sec_app_mort_acc__is_missing',\n",
    "    'sec_app_chargeoff_within_12_mths__is_missing',\n",
    "    'sec_app_earliest_cr_line__is_missing',\n",
    "    'sec_app_open_act_il__is_missing',\n",
    "    'dti_joint__is_missing',\n",
    "    'sec_app_open_acc__is_missing',\n",
    "    'sec_app_revol_util__is_missing',\n",
    "    'sec_app_num_rev_accts__is_missing',\n",
    "    'revol_bal_joint__is_missing',\n",
    "    'verification_status_joint__is_missing'\n",
    "]\n",
    "\n",
    "df_uncorr = df_pruned.drop(columns=drop_corr, errors=\"ignore\")\n",
    "print(\"Before:\", df_pruned.shape, \"After:\", df_uncorr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c5ff7-1753-4563-a744-e031dcdb921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncorr[target].value_counts(normalize=True).plot(kind=\"bar\")\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c769c69-027c-43a5-a77b-dd627c3c3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df_uncorr.select_dtypes(include=[\"number\",\"float32\",\"float64\",\"int32\",\"int64\"]).columns\n",
    "df_uncorr[num_cols].describe(percentiles=[0.01,0.05,0.95,0.99]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fc4b9-dddd-441a-a280-0289db90fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncorr.isna().mean(axis=1).plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Fraction of Missing per Row\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c109cec-46aa-48be-81ba-79008c3d94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df_uncorr.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "for col in cat_cols:\n",
    "    print(col, df_uncorr[col].nunique(), \"unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a78448-5903-4fc8-b4c0-8f15ad81324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = df_uncorr.drop(columns=[target]).copy()\n",
    "for c in X_enc.select_dtypes(include=[\"object\",\"category\"]).columns:\n",
    "    X_enc[c] = LabelEncoder().fit_transform(X_enc[c].astype(str))\n",
    "\n",
    "y_enc = df_uncorr[target]\n",
    "\n",
    "mi = mutual_info_classif(X_enc, y_enc, discrete_features=\"auto\", random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab577eff-ad73-4c05-8bb3-216a96fb3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi is your array; X_enc are the columns you used\n",
    "mi_df = (pd.DataFrame({\"feature\": X_enc.columns, \"mi\": mi})\n",
    "           .sort_values(\"mi\", ascending=False))\n",
    "display(mi_df.head(20))     # top signals\n",
    "display(mi_df.tail(20))     # weakest\n",
    "\n",
    "# Plot top-30\n",
    "mi_df.head(30).plot.bar(x=\"feature\", y=\"mi\", figsize=(10,4), rot=75)\n",
    "\n",
    "# Drop very weak features (e.g., MI ~ 0)\n",
    "weak_feats = mi_df.query(\"mi <= 1e-4\")[\"feature\"].tolist()\n",
    "df_uncorr2 = df_uncorr.drop(columns=weak_feats, errors=\"ignore\")\n",
    "print(\"Dropped:\", len(weak_feats), \"| New shape:\", df_uncorr2.shape)\n",
    "\n",
    "# Save for audit\n",
    "mi_df.to_csv(\"data/processed/mi_rank_100k.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb79551-db65-44c7-a544-18895a5ad375",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4CAF50; border-radius: 10px; padding: 15px; background-color: #f9fff9;\">\n",
    "\n",
    "## 🧹 Wrangling & EDA Summary  \n",
    "\n",
    "We started with **145 raw features** from the LendingClub loan dataset. Through systematic wrangling and exploratory data analysis, we reduced noise and redundancy to reach a **lean 80-feature dataset** ready for modeling.  \n",
    "\n",
    "### 🔎 Key Steps Taken  \n",
    "- **Dropped NA-heavy & constant columns**  \n",
    "  Columns with >99% missing values or no variance were removed.  \n",
    "\n",
    "- **Removed weak categorical features**  \n",
    "  Used Chi² tests and Mutual Information to identify and drop categorical variables with no measurable relationship to the target.  \n",
    "\n",
    "- **Pruned highly correlated features**  \n",
    "  Applied correlation heatmaps (Spearman) and removed strongly collinear indicators (e.g., multiple `__is_missing` flags for secondary applicants).  \n",
    "\n",
    "- **Filtered by Mutual Information**  \n",
    "  Calculated MI scores on a 100k subset and dropped features with near-zero predictive contribution.  \n",
    "\n",
    "### 📉 Feature Reduction  \n",
    "- Initial: **145 features**  \n",
    "- After cleaning + pruning: **80 features**  \n",
    "- **45% dimensionality reduction** while preserving signal  \n",
    "\n",
    "### ✅ Outcome  \n",
    "We now have a **lean, efficient dataset** that balances predictive potential with interpretability and compute efficiency. This forms the foundation for:  \n",
    "- More reliable EDA insights  \n",
    "- Faster model training  \n",
    "- Reduced risk of overfitting  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5087ec-d3b4-4abc-ac90-eb729a517980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncorr2[\"target_default\"] = df_uncorr2[\"loan_status\"].apply(label_default)\n",
    "\n",
    "print(df_uncorr2[\"target_default\"].value_counts(dropna=False, normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580dc6b-1513-4ecd-bd88-fdda4a2bf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_uncorr2.columns.tolist()[:30])   # first 30 cols\n",
    "print(df_uncorr2.select_dtypes(include=[\"number\"]).columns[:10])  # numeric sample\n",
    "print(df_uncorr2.select_dtypes(include=[\"category\",\"object\"]).columns[:10])  # categorical sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee762f47-ed37-40f0-9bb8-678342821ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target_default\"\n",
    "\n",
    "# 1) Categorical (binary) → countplot + default rate bars\n",
    "cat_feat = \"emp_length__is_missing\"  # 0/1 indicator you have\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.countplot(x=cat_feat, hue=target, data=df_uncorr2, order=[0,1])\n",
    "plt.title(f\"{cat_feat} vs {target}\")\n",
    "plt.xlabel(f\"{cat_feat} (0 = present, 1 = missing)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: default rate by category (probability bars)\n",
    "rate = (df_uncorr2.groupby(cat_feat)[target].mean() * 100).reindex([0,1])\n",
    "rate.plot(kind=\"bar\", figsize=(5,3), rot=0)\n",
    "plt.title(f\"P({target}=1) by {cat_feat}\")\n",
    "plt.ylabel(\"Default Rate (%)\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Numeric → boxplot vs target\n",
    "num_feat = \"int_rate\"  # numeric in your list\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.boxplot(x=target, y=num_feat, data=df_uncorr2)\n",
    "plt.title(f\"{num_feat} vs {target}\")\n",
    "plt.xlabel(target)\n",
    "plt.ylabel(num_feat)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99c79e-f60b-40e8-91d1-d041e61224d7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" >Group continuous features into bins, then calculate the probability of Default within each bin.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe2340-f1ba-481d-858a-cfe92fc378c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"target_default\"\n",
    "\n",
    "num_feats = [\"int_rate\", \"loan_amnt\", \"dti\", \"revol_util\", \"total_acc\", \"delinq_2yrs\"]\n",
    "cat_feats = [\"emp_length__is_missing\", \"annual_inc_joint__is_missing\", \"dti_joint__is_missing\"]\n",
    "\n",
    "plot_default_prob_grid(df_uncorr2, num_feats, target=target, n_cols=3, bins=5, strategy=\"quantile\")\n",
    "plot_default_prob_grid(df_uncorr2, cat_feats, target=target, n_cols=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8f5f5-c00e-4ded-9fd0-93d842f25349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
